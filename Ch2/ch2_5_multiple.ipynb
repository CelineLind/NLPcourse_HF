{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling multiple sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In the previous exercise you saw how sequences get translated into lists of numbers. Letâ€™s convert this list of numbers to a tensor and send it to the model:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 19:10:19.962948: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/celinelindeque/Documents/programming/ML/NLP/NLP/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-2.7276218,  2.8789387]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = tf.constant(ids)\n",
    "# This line will fail.\n",
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The problem is that we sent a single sequence to the model, whereas ðŸ¤— Transformers models expect multiple sentences by default.\"\n",
    "\n",
    "\"If you look closely, youâ€™ll see that the tokenizer didnâ€™t just convert the list of input IDs into a tensor, it added a dimension on top of it:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  101  1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026\n",
      "   2878  2166  1012   102]], shape=(1, 16), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tokenized_inputs = tokenizer(sequence, return_tensors=\"tf\")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tf.Tensor(\n",
      "[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878\n",
      "   2166  1012]], shape=(1, 14), dtype=int32)\n",
      "Logits: tf.Tensor([[-2.7276218  2.8789387]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Letâ€™s try again and add a new dimension:\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = tf.constant([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Batching is the act of sending multiple sentences through the model, all at once. If you only have one sentence, you can just build a batch with a single sequence:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tf.Tensor(\n",
      "[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878\n",
      "   2166  1012]\n",
      " [ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878\n",
      "   2166  1012]], shape=(2, 14), dtype=int32)\n",
      "Logits: tf.Tensor(\n",
      "[[-2.7276225  2.8789394]\n",
      " [-2.727621   2.878938 ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batched_ids = [ids, ids]\n",
    "batched_tensor = tf.constant(batched_ids)\n",
    "print(\"Input IDs:\", batched_tensor)\n",
    "output = model(batched_tensor)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs need to be the same length. For example, take this list of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They need to be padded into a rectangular shape.  \n",
    "\"Padding makes sure all our sentences have the same length by adding a special word called the padding token to the sentences with fewer values. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_id = 100\n",
    "\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, padding_id],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The padding token ID can be found in tokenizer.pad_token_id. Letâ€™s use it and send our two sentences through the model individually and batched together:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 1.5693687 -1.3894594]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[ 0.5802988  -0.41252303]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 1.5693679 -1.3894584]\n",
      " [ 1.3373474 -1.2163184]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(tf.constant(sequence1_ids)).logits)\n",
    "print(model(tf.constant(sequence2_ids)).logits)\n",
    "print(model(tf.constant(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the batched logits, you'd expect the second row of logits to be the same as the individual logits for sequence 2, but they aren't.  \n",
    "\"This is because the key feature of Transformer models is attention layers that contextualize each token. These will take into account the padding tokens since they attend to all of the tokens of a sequence. To get the same result when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, we need to tell those attention layers to ignore the padding tokens. This is done by using an attention mask.\"  \n",
    "* Tell the attention layers to ignore the padding tokens to get the same logits output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output logits from model:  tf.Tensor(\n",
      "[[ 1.5693679  -1.3894584 ]\n",
      " [ 0.58030313 -0.41252705]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor([[ 1.5693687 -1.3894594]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[ 0.5802988  -0.41252303]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))\n",
    "print(\"output logits from model: \",outputs.logits)\n",
    "print(model(tf.constant(sequence1_ids)).logits)\n",
    "print(model(tf.constant(sequence2_ids)).logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1045, 2293, 3698, 4083], [7976, 4454, 2003, 2126, 2488]]\n"
     ]
    }
   ],
   "source": [
    "sent1 = \"I love machine learning\"\n",
    "sent2 = \"artificial intelligence is way better\"\n",
    "\n",
    "tokens1 = tokenizer.tokenize(sent1)\n",
    "ids1 = tokenizer.convert_tokens_to_ids(tokens1)\n",
    "\n",
    "tokens2 = tokenizer.tokenize(sent2)\n",
    "ids2 = tokenizer.convert_tokens_to_ids(tokens2)\n",
    "\n",
    "batched_ids = [ids1, ids2]\n",
    "print(batched_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1045, 2293, 3698, 4083, 0]\n"
     ]
    }
   ],
   "source": [
    "new_ids1 = ids1.copy()\n",
    "new_ids1.append(tokenizer.pad_token_id) # append padding token to make equal lengths\n",
    "print(new_ids1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1045, 2293, 3698, 4083, 0], [7976, 4454, 2003, 2126, 2488]]\n"
     ]
    }
   ],
   "source": [
    "batched_ids_new = [new_ids1, ids2]\n",
    "print(batched_ids_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = [\n",
    "    [1, 1, 1, 1, 0],\n",
    "    [1, 1, 1, 1, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output logits from model:  tf.Tensor(\n",
      "[[-2.7265766  2.8925383]\n",
      " [ 3.191364  -2.592868 ]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor([[-2.7265766  2.8925393]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[ 3.1913645 -2.5928683]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "input_tensor = tf.constant(batched_ids_new)\n",
    "\n",
    "outputs = model(input_tensor, attention_mask=tf.constant(attention_mask))\n",
    "print(\"output logits from model: \",outputs.logits)\n",
    "\n",
    "print(model(tf.constant(ids1)).logits)\n",
    "print(model(tf.constant(ids2)).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longer Sequences  \n",
    "Models will often have a limit to how many tokens they can be passed. Most will handle up to 512 or 1014 tokens.  \n",
    "\n",
    "Solutions:  \n",
    "* Use a model which supports longer sequence lengths (and more tokens)\n",
    "* Truncate your sequences  \n",
    "\n",
    "Sequence: a sentence that has been tokenized (sequence length I think == number of tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we handle multiple sequences?  \n",
    "* Batch them  \n",
    "\n",
    "How do we handle multiple sequences of different lengths?  \n",
    "* Use paddings  \n",
    "\n",
    "Are vocabulary indices the only inputs that allow a model to work well?  \n",
    "* tbd.  \n",
    "\n",
    "Is there such a thing as too long a sequence?  \n",
    "* Yes, you can handle them by truncating your sequences, or using a model which can handle more"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
